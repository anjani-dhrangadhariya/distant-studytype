{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "532ec2aa",
   "metadata": {},
   "source": [
    "# LF summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68614907",
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum\n",
    "import glob\n",
    "import os\n",
    "from hashlib import new\n",
    "from pathlib import Path\n",
    "import time\n",
    "from itertools import product\n",
    "\n",
    "import functools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from snorkel.labeling.model import LabelModel as LMsnorkel\n",
    "from snorkel.labeling.model import MajorityLabelVoter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ebb6941",
   "metadata": {},
   "outputs": [],
   "source": [
    "candgen_version = 'v4' # version = {v3, v4, ...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "60e7428b",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity = 'S'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f31967f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import json\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "284d3de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09daa01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list2Nested(l, nested_length):\n",
    "    return [l[i:i+nested_length] for i in range(0, len(l), nested_length)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "959a6434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import scipy.sparse as sparse\n",
    "from scipy.sparse import issparse\n",
    "from pandas import DataFrame, Series\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "\n",
    "def arraylike_to_numpy(array_like):\n",
    "    \"\"\"Convert a 1d array-like (e.g,. list, tensor, etc.) to an np.ndarray\"\"\"\n",
    "    \n",
    "    orig_type = type(array_like)\n",
    "    \n",
    "    # Convert to np.ndarray\n",
    "    if isinstance(array_like, np.ndarray):\n",
    "        pass\n",
    "    elif isinstance(array_like, list):\n",
    "        array_like = np.array(array_like)\n",
    "    elif issparse(array_like):\n",
    "        array_like = array_like.toarray()\n",
    "    elif isinstance(array_like, torch.Tensor):\n",
    "        array_like = array_like.numpy()\n",
    "    elif not isinstance(array_like, np.ndarray):\n",
    "        array_like = np.array(array_like)\n",
    "    else:\n",
    "        msg = f\"Input of type {orig_type} could not be converted to 1d \" \"np.ndarray\"\n",
    "        raise ValueError(msg)\n",
    "    \n",
    "    # Correct shape\n",
    "    if (array_like.ndim > 1) and (1 in array_like.shape):\n",
    "        array_like = array_like.flatten()\n",
    "    if array_like.ndim != 1:\n",
    "        raise ValueError(\"Input could not be converted to 1d np.array\")\n",
    "    \n",
    "    # Convert to ints\n",
    "    if any(array_like % 1):\n",
    "        raise ValueError(\"Input contains at least one non-integer value.\")\n",
    "    array_like = array_like.astype(np.dtype(int))\n",
    "\n",
    "    return array_like\n",
    "\n",
    "\n",
    "############################################################\n",
    "# Label Matrix Diagnostics\n",
    "############################################################\n",
    "def _covered_data_points(L):\n",
    "    \"\"\"Returns an indicator vector where ith element = 1 if x_i is labeled by at\n",
    "    least one LF.\"\"\"\n",
    "    return np.ravel(np.where(L.sum(axis=1) != 0, 1, 0))\n",
    "\n",
    "\n",
    "def _overlapped_data_points(L):\n",
    "    \"\"\"Returns an indicator vector where ith element = 1 if x_i is labeled by\n",
    "    more than one LF.\"\"\"\n",
    "    return np.where(np.ravel((L != 0).sum(axis=1)) > 1, 1, 0)\n",
    "\n",
    "\n",
    "def _conflicted_data_points(L):\n",
    "    \"\"\"Returns an indicator vector where ith element = 1 if x_i is labeled by\n",
    "    at least two LFs that give it disagreeing labels.\"\"\"\n",
    "    m = sparse.diags(np.ravel(L.max(axis=1).todense()))\n",
    "    return np.ravel(np.max(m @ (L != 0) != L, axis=1).astype(int).todense())\n",
    "\n",
    "\n",
    "def label_coverage(L):\n",
    "    \"\"\"Returns the **fraction of data points with > 0 (non-zero) labels**\n",
    "    Args:\n",
    "        L: an n x m scipy.sparse matrix where L_{i,j} is the label given by the\n",
    "            jth LF to the ith item\n",
    "    \"\"\"\n",
    "    return _covered_data_points(L).sum() / L.shape[0]\n",
    "\n",
    "\n",
    "def label_overlap(L):\n",
    "    \"\"\"Returns the **fraction of data points with > 1 (non-zero) labels**\n",
    "    Args:\n",
    "        L: an n x m scipy.sparse matrix where L_{i,j} is the label given by the\n",
    "            jth LF to the ith item\n",
    "    \"\"\"\n",
    "    return _overlapped_data_points(L).sum() / L.shape[0]\n",
    "\n",
    "\n",
    "def label_conflict(L):\n",
    "    \"\"\"Returns the **fraction of data points with conflicting (disagreeing)\n",
    "    lablels.**\n",
    "    Args:\n",
    "        L: an n x m scipy.sparse matrix where L_{i,j} is the label given by the\n",
    "            jth LF to the ith item\n",
    "    \"\"\"\n",
    "    return _conflicted_data_points(L).sum() / L.shape[0]\n",
    "\n",
    "\n",
    "def lf_polarities(L):\n",
    "    \"\"\"Return the polarities of each LF based on evidence in a label matrix.\n",
    "\n",
    "    Args:\n",
    "        L: an n x m scipy.sparse matrix where L_{i,j} is the label given by the\n",
    "            jth LF to the ith candidate\n",
    "    \"\"\"\n",
    "    polarities = [sorted(list(set(L[:, i].data))) for i in range(L.shape[1])]\n",
    "    return [p[0] if len(p) == 1 else p for p in polarities]\n",
    "\n",
    "\n",
    "def lf_coverages(L):\n",
    "    \"\"\"Return the **fraction of data points that each LF labels.**\n",
    "    Args:\n",
    "        L: an n x m scipy.sparse matrix where L_{i,j} is the label given by the\n",
    "            jth LF to the ith candidate\n",
    "    \"\"\"\n",
    "    return np.ravel((L != 0).sum(axis=0)) / L.shape[0]\n",
    "\n",
    "\n",
    "def lf_raw_coverages(L):\n",
    "    \"\"\"Raw number of covered instances\"\"\"\n",
    "    return np.ravel((L != 0).sum(axis=0))\n",
    "\n",
    "\n",
    "def lf_overlaps(L, normalize_by_coverage=False):\n",
    "    \"\"\"Return the **fraction of items each LF labels that are also labeled by at\n",
    "     least one other LF.**\n",
    "\n",
    "    Note that the maximum possible overlap fraction for an LF is the LF's\n",
    "    coverage, unless `normalize_by_coverage=True`, in which case it is 1.\n",
    "\n",
    "    Args:\n",
    "        L: an n x m scipy.sparse matrix where L_{i,j} is the label given by the\n",
    "            jth LF to the ith candidate\n",
    "        normalize_by_coverage: Normalize by coverage of the LF, so that it\n",
    "            returns the percent of LF labels that have overlaps.\n",
    "    \"\"\"\n",
    "    overlaps = (L != 0).T @ _overlapped_data_points(L) / L.shape[0]\n",
    "    if normalize_by_coverage:\n",
    "        overlaps /= lf_coverages(L)\n",
    "    return np.nan_to_num(overlaps)\n",
    "\n",
    "\n",
    "def lf_conflicts(L, normalize_by_overlaps=False):\n",
    "    \"\"\"Return the **fraction of items each LF labels that are also given a\n",
    "    different (non-abstain) label by at least one other LF.**\n",
    "\n",
    "    Note that the maximum possible conflict fraction for an LF is the LF's\n",
    "        overlaps fraction, unless `normalize_by_overlaps=True`, in which case it\n",
    "        is 1.\n",
    "\n",
    "    Args:\n",
    "        L: an n x m scipy.sparse matrix where L_{i,j} is the label given by the\n",
    "            jth LF to the ith candidate\n",
    "        normalize_by_overlaps: Normalize by overlaps of the LF, so that it\n",
    "            returns the percent of LF overlaps that have conflicts.\n",
    "    \"\"\"\n",
    "    conflicts = (L != 0).T @ _conflicted_data_points(L) / L.shape[0]\n",
    "    if normalize_by_overlaps:\n",
    "        conflicts /= lf_overlaps(L)\n",
    "    return np.nan_to_num(conflicts)\n",
    "\n",
    "\n",
    "\n",
    "def lf_empirical_accuracies(L, Y):\n",
    "    \"\"\"Return the **empirical accuracy** against a set of labels Y (e.g. dev\n",
    "    set) for each LF.\n",
    "    Args:\n",
    "        L: an n x m scipy.sparse matrix where L_{i,j} is the label given by the\n",
    "            jth LF to the ith candidate\n",
    "        Y: an [n] or [n, 1] np.ndarray of gold labels\n",
    "    \"\"\"\n",
    "    # Assume labeled set is small, work with dense matrices\n",
    "    Y = arraylike_to_numpy(Y)\n",
    "    L = L.toarray()\n",
    "    X = np.where(\n",
    "        L == 0,\n",
    "        0,\n",
    "        np.where(L == np.vstack([Y] * L.shape[1]).T, 1, -1)\n",
    "    )\n",
    "    return 0.5 * (X.sum(axis=0) / (L != 0).sum(axis=0) + 1)\n",
    "\n",
    "\n",
    "def lf_summary(L, Y=None, lf_names=None, est_accs=None):\n",
    "    \"\"\"Returns a pandas DataFrame with the various per-LF statistics.\n",
    "\n",
    "    Args:\n",
    "        L: an n x m scipy.sparse matrix where L_{i,j} is the label given by the\n",
    "            jth LF to the ith candidate\n",
    "        Y: an [n] or [n, 1] np.ndarray of gold labels.\n",
    "            If provided, the empirical accuracy for each LF will be calculated\n",
    "    \"\"\"\n",
    "    n, m = L.shape\n",
    "    if lf_names is not None:\n",
    "        col_names = [\"j\"]\n",
    "        d = {\"j\": list(range(m))}\n",
    "    else:\n",
    "        lf_names = list(range(m))\n",
    "        col_names = []\n",
    "        d = {}\n",
    "\n",
    "    # Default LF stats\n",
    "    col_names.extend([\"Polarity\", \"Coverage%\", \"Overlaps%\", \"Conflicts%\", \"Coverage\"])\n",
    "    d[\"Polarity\"] = Series(data=lf_polarities(L), index=lf_names)\n",
    "    d[\"Coverage%\"] = Series(data=lf_coverages(L), index=lf_names)\n",
    "    d[\"Overlaps%\"] = Series(data=lf_overlaps(L), index=lf_names)\n",
    "    d[\"Conflicts%\"] = Series(data=lf_conflicts(L), index=lf_names)\n",
    "\n",
    "    d[\"Coverage\"] = Series(data=lf_raw_coverages(L), index=lf_names)\n",
    "    \n",
    "\n",
    "    if Y is not None:\n",
    "        col_names.extend([\"Correct\", \"Incorrect\", \"Emp. Acc.\"])\n",
    "        confusions = [\n",
    "            confusion_matrix(Y, L[:, i], pretty_print=False) for i in range(m)\n",
    "        ]\n",
    "        corrects = [np.diagonal(conf).sum() for conf in confusions]\n",
    "        incorrects = [\n",
    "            conf.sum() - correct for conf, correct in zip(confusions, corrects)\n",
    "        ]\n",
    "        accs = lf_empirical_accuracies(L, Y)\n",
    "        d[\"Correct\"] = Series(data=corrects, index=lf_names)\n",
    "        d[\"Incorrect\"] = Series(data=incorrects, index=lf_names)\n",
    "        d[\"Emp. Acc.\"] = Series(data=accs, index=lf_names)\n",
    "\n",
    "    if est_accs is not None:\n",
    "        col_names.append(\"Learned Acc.\")\n",
    "        d[\"Learned Acc.\"] = Series(est_accs, index=lf_names)\n",
    "\n",
    "    return DataFrame(data=d, index=lf_names)[col_names]\n",
    "\n",
    "\n",
    "def single_lf_summary(Y_p, Y=None):\n",
    "    \"\"\"Calculates coverage, overlap, conflicts, and accuracy for a single LF\n",
    "\n",
    "    Args:\n",
    "        Y_p: a np.array or torch.Tensor of predicted labels\n",
    "        Y: a np.array or torch.Tensor of true labels (if known)\n",
    "    \"\"\"\n",
    "    L = sparse.csr_matrix(arraylike_to_numpy(Y_p).reshape(-1, 1))\n",
    "    return lf_summary(L, Y)\n",
    "\n",
    "\n",
    "def error_buckets(gold, pred, X=None):\n",
    "    \"\"\"Group items by error buckets\n",
    "\n",
    "    Args:\n",
    "        gold: an array-like of gold labels (ints)\n",
    "        pred: an array-like of predictions (ints)\n",
    "        X: an iterable of items\n",
    "    Returns:\n",
    "        buckets: A dict of items where buckets[i,j] is a list of items with\n",
    "            predicted label i and true label j. If X is None, return indices\n",
    "            instead.\n",
    "\n",
    "    For a binary problem with (1=positive, 2=negative):\n",
    "        buckets[1,1] = true positives\n",
    "        buckets[1,2] = false positives\n",
    "        buckets[2,1] = false negatives\n",
    "        buckets[2,2] = true negatives\n",
    "    \"\"\"\n",
    "    buckets = defaultdict(list)\n",
    "    gold = arraylike_to_numpy(gold)\n",
    "    pred = arraylike_to_numpy(pred)\n",
    "    for i, (y, l) in enumerate(zip(pred, gold)):\n",
    "        buckets[y, l].append(X[i] if X is not None else i)\n",
    "    return buckets\n",
    "\n",
    "\n",
    "def confusion_matrix(\n",
    "    gold, pred, null_pred=False, null_gold=False, normalize=False, pretty_print=True\n",
    "):\n",
    "    \"\"\"A shortcut method for building a confusion matrix all at once.\n",
    "\n",
    "    Args:\n",
    "        gold: an array-like of gold labels (ints)\n",
    "        pred: an array-like of predictions (ints)\n",
    "        null_pred: If True, include the row corresponding to null predictions\n",
    "        null_gold: If True, include the col corresponding to null gold labels\n",
    "        normalize: if True, divide counts by the total number of items\n",
    "        pretty_print: if True, pretty-print the matrix before returning\n",
    "    \"\"\"\n",
    "    conf = ConfusionMatrix(null_pred=null_pred, null_gold=null_gold)\n",
    "    gold = arraylike_to_numpy(gold)\n",
    "    pred = arraylike_to_numpy(pred)\n",
    "    conf.add(gold, pred)\n",
    "    mat = conf.compile()\n",
    "\n",
    "    if normalize:\n",
    "        mat = mat / len(gold)\n",
    "\n",
    "    if pretty_print:\n",
    "        conf.display(normalize=normalize)\n",
    "\n",
    "    return mat\n",
    "\n",
    "\n",
    "class ConfusionMatrix(object):\n",
    "    \"\"\"\n",
    "    An iteratively built abstention-aware confusion matrix with pretty printing\n",
    "\n",
    "    Assumed axes are true label on top, predictions on the side.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, null_pred=False, null_gold=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            null_pred: If True, include the row corresponding to null\n",
    "                predictions\n",
    "            null_gold: If True, include the col corresponding to null gold\n",
    "                labels\n",
    "\n",
    "        \"\"\"\n",
    "        self.counter = Counter()\n",
    "        self.mat = None\n",
    "        self.null_pred = null_pred\n",
    "        self.null_gold = null_gold\n",
    "\n",
    "    def __repr__(self):\n",
    "        if self.mat is None:\n",
    "            self.compile()\n",
    "        return str(self.mat)\n",
    "\n",
    "    def add(self, gold, pred):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            gold: a np.ndarray of gold labels (ints)\n",
    "            pred: a np.ndarray of predictions (ints)\n",
    "        \"\"\"\n",
    "        self.counter.update(zip(gold, pred))\n",
    "\n",
    "    def compile(self, trim=True):\n",
    "        k = max([max(tup) for tup in self.counter.keys()]) + 1  # include 0\n",
    "\n",
    "        mat = np.zeros((k, k), dtype=int)\n",
    "        for (y, l), v in self.counter.items():\n",
    "            mat[l, y] = v\n",
    "\n",
    "        if trim and not self.null_pred:\n",
    "            mat = mat[1:, :]\n",
    "        if trim and not self.null_gold:\n",
    "            mat = mat[:, 1:]\n",
    "\n",
    "        self.mat = mat\n",
    "        return mat\n",
    "\n",
    "    def display(self, normalize=False, indent=0, spacing=2, decimals=3, mark_diag=True):\n",
    "        mat = self.compile(trim=False)\n",
    "        m, n = mat.shape\n",
    "        tab = \" \" * spacing\n",
    "        margin = \" \" * indent\n",
    "\n",
    "        # Print headers\n",
    "        s = margin + \" \" * (5 + spacing)\n",
    "        for j in range(n):\n",
    "            if j == 0 and not self.null_gold:\n",
    "                continue\n",
    "            s += f\" y={j} \" + tab\n",
    "        print(s)\n",
    "\n",
    "        # Print data\n",
    "        for i in range(m):\n",
    "            # Skip null predictions row if necessary\n",
    "            if i == 0 and not self.null_pred:\n",
    "                continue\n",
    "            s = margin + f\" l={i} \" + tab\n",
    "            for j in range(n):\n",
    "                # Skip null gold if necessary\n",
    "                if j == 0 and not self.null_gold:\n",
    "                    continue\n",
    "                else:\n",
    "                    if i == j and mark_diag and normalize:\n",
    "                        s = s[:-1] + \"*\"\n",
    "                    if normalize:\n",
    "                        s += f\"{mat[i,j]/sum(mat[i,1:]):>5.3f}\" + tab\n",
    "                    else:\n",
    "                        s += f\"{mat[i,j]:^5d}\" + tab\n",
    "            print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c77b31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from LabelModelTrain import LMutils\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "457d5155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapTrueLabels(l):\n",
    "    \n",
    "    updated_values = []\n",
    "    for l_i in l:\n",
    "        updated_values.append( label_mapper[l_i] )\n",
    "        \n",
    "    return updated_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42bbf213",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = f'/mnt/nas2/results/Results/systematicReview/distant_pico/EBM_PICO_GT/{candgen_version}/gt/train_ebm_labels_tui_pio3.tsv'\n",
    "training_data = pd.read_csv(train_file, sep='\\t', header=0)\n",
    "training_data.rename( columns={'Unnamed: 0':'series'}, inplace=True )\n",
    "\n",
    "val_file = f'/mnt/nas2/results/Results/systematicReview/distant_pico/EBM_PICO_GT/{candgen_version}/gt/val_studytype_tui_pio3.tsv'\n",
    "val_data = pd.read_csv(val_file, sep='\\t', header=0)\n",
    "val_data.rename( columns={'Unnamed: 0':'series'}, inplace=True )\n",
    "\n",
    "ebm_test_file = f'/mnt/nas2/results/Results/systematicReview/distant_pico/EBM_PICO_GT/{candgen_version}/gt/test_ebm_labels_tui_pio3.tsv'\n",
    "test_ebm_data = pd.read_csv(ebm_test_file, sep='\\t', header=0)\n",
    "test_ebm_data.rename( columns={'Unnamed: 0':'series'}, inplace=True )\n",
    "\n",
    "physio_test_file = f'/mnt/nas2/results/Results/systematicReview/distant_pico/EBM_PICO_GT/{candgen_version}/gt/test_physio_labels_tui_pio3.tsv'\n",
    "test_physio_data = pd.read_csv(physio_test_file, sep='\\t', header=0)\n",
    "test_physio_data.rename( columns={'Unnamed: 0':'series'}, inplace=True )\n",
    "\n",
    "ebm_test_corrected_file = f'/mnt/nas2/results/Results/systematicReview/distant_pico/EBM_PICO_GT/{candgen_version}/gt/test_ebm_st_corr_tui_pio3.tsv'\n",
    "test_ebm_corrected_data = pd.read_csv(ebm_test_corrected_file, sep='\\t', header=0)\n",
    "test_ebm_corrected_data.rename( columns={'Unnamed: 0':'series'}, inplace=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee9030d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter Training set to remove Validation set IDs\n",
    "\n",
    "cond = training_data['pmid'].isin(val_data['pmid'])\n",
    "training_data.drop(training_data[cond].index, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "067c2fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_df(df):\n",
    "\n",
    "    df_series = [ index for index, value in df.tokens.items() for word in ast.literal_eval(value) ]\n",
    "    df_tokens = [ word for index, value in df.tokens.items() for word in ast.literal_eval(value) ]\n",
    "    df_pos = [ word for index, value in df.pos.items() for word in ast.literal_eval(value) ]\n",
    "    df_offsets = [ word for index, value in df.offsets.items() for word in ast.literal_eval(value) ]\n",
    "\n",
    "\n",
    "    df_p = [ int(lab) for index, value in df.p.items() for lab in ast.literal_eval(value) ]\n",
    "    df_p_fine = [ int(lab) for index, value in df.p_f.items() for lab in ast.literal_eval(value) ]\n",
    "    df_i = [ int(lab) for index, value in df.i.items() for lab in ast.literal_eval(value) ]\n",
    "    df_i_fine = [ int(lab) for index, value in df.i_f.items() for lab in ast.literal_eval(value) ]\n",
    "    df_o = [ int(lab) for index, value in df.o.items() for lab in ast.literal_eval(value) ]\n",
    "    df_o_fine = [ int(lab) for index, value in df.o_f.items() for lab in ast.literal_eval(value) ]\n",
    "    df_s = [ int(lab) for index, value in df.s.items() for lab in ast.literal_eval(value) ]\n",
    "    df_s_fine = [ int(lab) for index, value in df.s_f.items() for lab in ast.literal_eval(value) ]\n",
    "    \n",
    "    df_flattened = pd.DataFrame({ 'series': df_series,\n",
    "                        'tokens' : df_tokens,\n",
    "                        'offsets': df_offsets,\n",
    "                        'pos': df_pos,\n",
    "                        'p' : df_p,\n",
    "                        'i' : df_i,\n",
    "                        'o' : df_o,\n",
    "                        's' : df_s,\n",
    "                        'p_f' : df_p_fine,\n",
    "                        'i_f' : df_i_fine,\n",
    "                        'o_f' : df_o_fine,\n",
    "                        's_f' : df_s_fine})\n",
    "    \n",
    "    return df_flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73bad584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the dataframes (currently only the training dataframe and test ebm dataframe with corrected labels can be flattened)\n",
    "data_df = flatten_df(training_data)\n",
    "val_df = flatten_df(val_data)\n",
    "test_ebm_data = flatten_df(test_ebm_data)\n",
    "test_ebm_corr_df = flatten_df(test_ebm_corrected_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96306050",
   "metadata": {},
   "outputs": [],
   "source": [
    "series = [\n",
    "    data_df.series.to_numpy() ,\n",
    "    val_df.series.to_numpy() ,\n",
    "    test_ebm_data.series.to_numpy() ,\n",
    "    test_physio_data.series.to_numpy(),   \n",
    "    test_ebm_corr_df.series.to_numpy()\n",
    "]\n",
    "\n",
    "\n",
    "sents = [\n",
    "    data_df.tokens.to_numpy() ,\n",
    "    val_df.tokens.to_numpy() ,\n",
    "    test_ebm_data.tokens.to_numpy() ,\n",
    "    test_physio_data.tokens.to_numpy(),   \n",
    "    test_ebm_corr_df.tokens.to_numpy()    \n",
    "]\n",
    "\n",
    "\n",
    "part_of_speech = [\n",
    "    data_df.pos.to_numpy() ,\n",
    "    val_df.pos.to_numpy() ,\n",
    "    test_ebm_data.pos.to_numpy() ,\n",
    "    test_physio_data.pos.to_numpy(),   \n",
    "    test_ebm_corr_df.pos.to_numpy()     \n",
    "]\n",
    "\n",
    "\n",
    "offsets = [\n",
    "    data_df.offsets.to_numpy() ,\n",
    "    val_df.offsets.to_numpy() ,\n",
    "    test_ebm_data.offsets.to_numpy() ,\n",
    "    test_physio_data.offsets.to_numpy(),   \n",
    "    test_ebm_corr_df.offsets.to_numpy() \n",
    "]\n",
    "\n",
    "\n",
    "Y_p = [\n",
    "    data_df.p.to_numpy() , # 0 -9\n",
    "    data_df.p_f.to_numpy() , # 1 -8\n",
    "    val_df.p.to_numpy() , # 2 -7\n",
    "    val_df.p_f.to_numpy() , # 3 -6\n",
    "    test_ebm_data.p.to_numpy() , # 4 -5\n",
    "    test_ebm_data.p_f.to_numpy() , # 5 -4\n",
    "    test_physio_data.p.to_numpy(),  # 6 -3\n",
    "    test_ebm_corr_df.p.to_numpy(),   # 7 -2\n",
    "    test_ebm_corr_df.p_f.to_numpy() # 8 -1\n",
    "]\n",
    "\n",
    "\n",
    "Y_i = [\n",
    "    data_df.i.to_numpy() , # 0 -9\n",
    "    data_df.i_f.to_numpy() , # 1 -8\n",
    "    val_df.i.to_numpy() , # 2 -7\n",
    "    val_df.i_f.to_numpy() , # 3 -6\n",
    "    test_ebm_data.i.to_numpy() , # 4 -5\n",
    "    test_ebm_data.i_f.to_numpy() , # 5 -4\n",
    "    test_physio_data.i.to_numpy(),  # 6 -3\n",
    "    test_ebm_corr_df.i.to_numpy(),   # 7 -2\n",
    "    test_ebm_corr_df.i_f.to_numpy() # 8 -1\n",
    "]\n",
    "\n",
    "\n",
    "Y_o = [\n",
    "    data_df.o.to_numpy() , # 0 -9\n",
    "    data_df.o_f.to_numpy() , # 1 -8\n",
    "    val_df.o.to_numpy() , # 2 -7\n",
    "    val_df.o_f.to_numpy() , # 3 -6\n",
    "    test_ebm_data.o.to_numpy() , # 4 -5\n",
    "    test_ebm_data.o_f.to_numpy() , # 5 -4\n",
    "    test_physio_data.o.to_numpy(),  # 6 -3\n",
    "    test_ebm_corr_df.o.to_numpy(),   # 7 -2\n",
    "    test_ebm_corr_df.o_f.to_numpy() # 8 -1\n",
    "]\n",
    "\n",
    "\n",
    "Y_s = [\n",
    "    data_df.s.to_numpy() , # 0 -9\n",
    "    data_df.s_f.to_numpy() , # 1 -8\n",
    "    val_df.s.to_numpy() , # 2 -7\n",
    "    val_df.s_f.to_numpy() , # 3 -6\n",
    "    test_ebm_data.s.to_numpy() , # 4 -5\n",
    "    test_ebm_data.s_f.to_numpy() , # 5 -4\n",
    "    test_physio_data.s.to_numpy(),  # 6 -3\n",
    "    test_ebm_corr_df.s.to_numpy(),   # 7 -2\n",
    "    test_ebm_corr_df.s_f.to_numpy() # 8 -1\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "230128ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_list(data_column):\n",
    "    return [ word for index, value in data_column.items() for word in ast.literal_eval(value) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dcf8fceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_array(data_column):\n",
    "    return np.array( [ word for index, value in data_column.items() for word in ast.literal_eval(value) ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8b67eb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_array(label_column):\n",
    "    return np.array( [ labelModel_mapper_LF[int(lab)] for index, value in label_column.items() for k, lab in ast.literal_eval(value).items() ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7f8f76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1:1 positive to positive\n",
    "# -1:0 negative cand_gen to negative in label model\n",
    "# 0:-1 Abstain cand_gen to abstain in label model\n",
    "\n",
    "# In study type, abstain is actually a negative instance \n",
    "#labelModel_mapper_LF = {1:1, 0:0, -1:-1}\n",
    "#labelModel_mapper_LF = {1:1, -1:0, 0:-1}\n",
    "\n",
    "label_mapper_GT = {1:1, 0:2}\n",
    "#labelModel_mapper_LF = {1:1, -1:2, 0:0}\n",
    "labelModel_mapper_LF = {1:1, 0:0, -1:-1}\n",
    "\n",
    "\n",
    "\n",
    "#if picos == 'S':\n",
    "#    labelModel_mapper_LF = {1:1, 0:2, -1:0}\n",
    "#else:\n",
    "#    labelModel_mapper_LF = {1:1, -1:2, 0:0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9362c81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lfs(indir, invalid_pmids = None):\n",
    "    \n",
    "    pathlist = Path(indir).glob('**/*.tsv')\n",
    "\n",
    "    tokens = ''\n",
    "\n",
    "    lfs = dict()\n",
    "    lfs_lm = dict()\n",
    "\n",
    "    for counter, file in enumerate(pathlist):\n",
    "        \n",
    "        if f'/{entity}/' in str(file):\n",
    "\n",
    "            k = str( file ).split(f'/{candgen_version}/')[-1].replace('.tsv', '').replace('/', '_')\n",
    "            mypath = Path(file)\n",
    "            if mypath.stat().st_size != 0:\n",
    "                data = pd.read_csv(file, sep='\\t', header=0)\n",
    "                \n",
    "                # Remove validation PMIDs\n",
    "                if invalid_pmids is not None:\n",
    "                    cond = data['pmid'].isin(val_data['pmid'])\n",
    "                    data.drop(data[cond].index, inplace = True)\n",
    "\n",
    "                data_tokens = data.tokens\n",
    "                if len(tokens) < 5:\n",
    "                    tokens = df_to_array(data_tokens)\n",
    "\n",
    "                data_labels = data.labels\n",
    "                \n",
    "                labels = dict_to_array(data_labels)\n",
    "                \n",
    "                if len(labels) != len(tokens):\n",
    "                    print(k, len(labels) , len(tokens) )\n",
    "                #assert len(labels) == len(tokens)\n",
    "                lfs[k] = labels\n",
    "\n",
    "\n",
    "    print( 'Total number of tokens in data set: ', len(tokens) )\n",
    "    print( 'Total number of LFs in the dictionary', len(lfs) )\n",
    "    \n",
    "    return lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ebc41660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens in data set:  1102837\n",
      "Total number of LFs in the dictionary 268\n"
     ]
    }
   ],
   "source": [
    "indir = f'/mnt/nas2/results/Results/systematicReview/distant_pico/tui_pio_v4/training_ebm_candidate_generation/{candgen_version}'\n",
    "train_ebm_lfs = get_lfs(indir, invalid_pmids = val_data.pmid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8640226c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens in data set:  211870\n",
      "Total number of LFs in the dictionary 268\n"
     ]
    }
   ],
   "source": [
    "indir_val_ebm = f'/mnt/nas2/results/Results/systematicReview/distant_pico/tui_pio_v4/val_studytype_candidate_generation/{candgen_version}'\n",
    "val_ebm_lfs = get_lfs(indir_val_ebm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "61566874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens in data set:  52582\n",
      "Total number of LFs in the dictionary 268\n"
     ]
    }
   ],
   "source": [
    "indir_test_ebm_corr = f'/mnt/nas2/results/Results/systematicReview/distant_pico/tui_pio_v4/test_ebm_anjani_candidate_generation/{candgen_version}'\n",
    "test_ebm_corr_lfs = get_lfs(indir_test_ebm_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d3595886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens in data set:  51784\n",
      "Total number of LFs in the dictionary 268\n"
     ]
    }
   ],
   "source": [
    "indir_test_ebm = f'/mnt/nas2/results/Results/systematicReview/distant_pico/tui_pio_v4/test_ebm_candidate_generation/{candgen_version}'\n",
    "test_ebm_lfs = get_lfs(indir_test_ebm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0b3998aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop some lfs\n",
    "def drop_nopositive(lfs_d):\n",
    "    \n",
    "    dropped_conditions = dict()\n",
    "\n",
    "    for k, v in lfs_d.items():\n",
    "        \n",
    "        #if '_negs' not in str(k):\n",
    "        dropped_conditions[k] = v\n",
    "        \n",
    "        '''       \n",
    "        if '_negs' not in str(k):\n",
    "            \n",
    "\n",
    "            if 'stdtype_types' in str(k):\n",
    "                print(k)\n",
    "                k_mod = str(k) + '_'\n",
    "                dropped_conditions[k_mod] = v\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "        '''\n",
    "            \n",
    "    return dropped_conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "85212a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ebm_lfs_dropped = drop_nopositive(train_ebm_lfs)\n",
    "val_ebm_lfs_dropped = drop_nopositive(val_ebm_lfs)\n",
    "test_ebm_corr_lfs_dropped = drop_nopositive(test_ebm_corr_lfs)\n",
    "test_ebm_lfs_dropped = drop_nopositive(test_ebm_lfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "80f6aeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dict_lfs = pd.DataFrame({ key:pd.Series(value) for key, value in val_ebm_lfs.items() })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "229f9421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map \n",
    "label_mapper_GT = {1:1, 0:2}\n",
    "\n",
    "val_y_mapped = [ label_mapper_GT[i] for i in val_df.s_f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "771b610b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_lf_names = [*val_dict_lfs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e3cbeeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSummary_and_write(df, df_col_head, true_labels, file_path):\n",
    "    \n",
    "    #convert to sciy matrix\n",
    "    scipy_mat = scipy.sparse.csr_matrix(df.values)\n",
    "    \n",
    "    lf_summary_df = lf_summary(scipy_mat, Y=true_labels, lf_names=df_col_head)\n",
    "    \n",
    "    lf_summary_df.to_csv(file_path, sep='\\t')\n",
    "    \n",
    "    return lf_summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cf9981db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-51d869ceabbc>:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return 0.5 * (X.sum(axis=0) / (L != 0).sum(axis=0) + 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage%</th>\n",
       "      <th>Overlaps%</th>\n",
       "      <th>Conflicts%</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Correct</th>\n",
       "      <th>Incorrect</th>\n",
       "      <th>Emp. Acc.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dictionary_fuzzy_S_lf_dict_s_type_negs</th>\n",
       "      <td>0</td>\n",
       "      <td>[-1, 1]</td>\n",
       "      <td>0.029636</td>\n",
       "      <td>0.029636</td>\n",
       "      <td>0.029570</td>\n",
       "      <td>6279</td>\n",
       "      <td>6205</td>\n",
       "      <td>74</td>\n",
       "      <td>0.143813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dictionary_fuzzy_S_lf_dict_s_comp_type_negs</th>\n",
       "      <td>1</td>\n",
       "      <td>[-1, 1]</td>\n",
       "      <td>0.030552</td>\n",
       "      <td>0.030552</td>\n",
       "      <td>0.030283</td>\n",
       "      <td>6473</td>\n",
       "      <td>6181</td>\n",
       "      <td>292</td>\n",
       "      <td>0.137958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dictionary_direct_S_lf_dict_s_type_negs</th>\n",
       "      <td>2</td>\n",
       "      <td>[-1, 1]</td>\n",
       "      <td>0.029636</td>\n",
       "      <td>0.029636</td>\n",
       "      <td>0.029570</td>\n",
       "      <td>6279</td>\n",
       "      <td>6205</td>\n",
       "      <td>74</td>\n",
       "      <td>0.143813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dictionary_direct_S_lf_dict_s_comp_type_negs</th>\n",
       "      <td>3</td>\n",
       "      <td>[-1, 1]</td>\n",
       "      <td>0.030552</td>\n",
       "      <td>0.030552</td>\n",
       "      <td>0.030283</td>\n",
       "      <td>6473</td>\n",
       "      <td>6181</td>\n",
       "      <td>292</td>\n",
       "      <td>0.137958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UMLS_fuzzy_S_lf_PCDS</th>\n",
       "      <td>4</td>\n",
       "      <td>[-1, 1]</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.001525</td>\n",
       "      <td>324</td>\n",
       "      <td>309</td>\n",
       "      <td>15</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heuristics_direct_S_lf_regex_blinding_negs</th>\n",
       "      <td>263</td>\n",
       "      <td>[-1, 1]</td>\n",
       "      <td>0.376618</td>\n",
       "      <td>0.376618</td>\n",
       "      <td>0.374550</td>\n",
       "      <td>79794</td>\n",
       "      <td>77159</td>\n",
       "      <td>2635</td>\n",
       "      <td>0.005389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heuristics_direct_S_lf_regex_stdtype_basicplus_negs</th>\n",
       "      <td>264</td>\n",
       "      <td>[-1, 1]</td>\n",
       "      <td>0.375943</td>\n",
       "      <td>0.375943</td>\n",
       "      <td>0.374555</td>\n",
       "      <td>79651</td>\n",
       "      <td>76990</td>\n",
       "      <td>2661</td>\n",
       "      <td>0.003277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heuristics_direct_S_lf_lf_lf_s_heurpattern_labels_2</th>\n",
       "      <td>265</td>\n",
       "      <td>1</td>\n",
       "      <td>0.025501</td>\n",
       "      <td>0.025501</td>\n",
       "      <td>0.017407</td>\n",
       "      <td>5403</td>\n",
       "      <td>2860</td>\n",
       "      <td>2543</td>\n",
       "      <td>0.529336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heuristics_direct_S_lf_regex_blinding</th>\n",
       "      <td>266</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003804</td>\n",
       "      <td>0.003804</td>\n",
       "      <td>0.001737</td>\n",
       "      <td>806</td>\n",
       "      <td>784</td>\n",
       "      <td>22</td>\n",
       "      <td>0.972705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heuristics_direct_S_lf_regex_stdtype_basicplus</th>\n",
       "      <td>267</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005711</td>\n",
       "      <td>0.005711</td>\n",
       "      <td>0.004323</td>\n",
       "      <td>1210</td>\n",
       "      <td>1123</td>\n",
       "      <td>87</td>\n",
       "      <td>0.928099</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>268 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      j Polarity  Coverage%  \\\n",
       "dictionary_fuzzy_S_lf_dict_s_type_negs                0  [-1, 1]   0.029636   \n",
       "dictionary_fuzzy_S_lf_dict_s_comp_type_negs           1  [-1, 1]   0.030552   \n",
       "dictionary_direct_S_lf_dict_s_type_negs               2  [-1, 1]   0.029636   \n",
       "dictionary_direct_S_lf_dict_s_comp_type_negs          3  [-1, 1]   0.030552   \n",
       "UMLS_fuzzy_S_lf_PCDS                                  4  [-1, 1]   0.001529   \n",
       "...                                                 ...      ...        ...   \n",
       "heuristics_direct_S_lf_regex_blinding_negs          263  [-1, 1]   0.376618   \n",
       "heuristics_direct_S_lf_regex_stdtype_basicplus_...  264  [-1, 1]   0.375943   \n",
       "heuristics_direct_S_lf_lf_lf_s_heurpattern_labe...  265        1   0.025501   \n",
       "heuristics_direct_S_lf_regex_blinding               266        1   0.003804   \n",
       "heuristics_direct_S_lf_regex_stdtype_basicplus      267        1   0.005711   \n",
       "\n",
       "                                                    Overlaps%  Conflicts%  \\\n",
       "dictionary_fuzzy_S_lf_dict_s_type_negs               0.029636    0.029570   \n",
       "dictionary_fuzzy_S_lf_dict_s_comp_type_negs          0.030552    0.030283   \n",
       "dictionary_direct_S_lf_dict_s_type_negs              0.029636    0.029570   \n",
       "dictionary_direct_S_lf_dict_s_comp_type_negs         0.030552    0.030283   \n",
       "UMLS_fuzzy_S_lf_PCDS                                 0.001529    0.001525   \n",
       "...                                                       ...         ...   \n",
       "heuristics_direct_S_lf_regex_blinding_negs           0.376618    0.374550   \n",
       "heuristics_direct_S_lf_regex_stdtype_basicplus_...   0.375943    0.374555   \n",
       "heuristics_direct_S_lf_lf_lf_s_heurpattern_labe...   0.025501    0.017407   \n",
       "heuristics_direct_S_lf_regex_blinding                0.003804    0.001737   \n",
       "heuristics_direct_S_lf_regex_stdtype_basicplus       0.005711    0.004323   \n",
       "\n",
       "                                                    Coverage  Correct  \\\n",
       "dictionary_fuzzy_S_lf_dict_s_type_negs                  6279     6205   \n",
       "dictionary_fuzzy_S_lf_dict_s_comp_type_negs             6473     6181   \n",
       "dictionary_direct_S_lf_dict_s_type_negs                 6279     6205   \n",
       "dictionary_direct_S_lf_dict_s_comp_type_negs            6473     6181   \n",
       "UMLS_fuzzy_S_lf_PCDS                                     324      309   \n",
       "...                                                      ...      ...   \n",
       "heuristics_direct_S_lf_regex_blinding_negs             79794    77159   \n",
       "heuristics_direct_S_lf_regex_stdtype_basicplus_...     79651    76990   \n",
       "heuristics_direct_S_lf_lf_lf_s_heurpattern_labe...      5403     2860   \n",
       "heuristics_direct_S_lf_regex_blinding                    806      784   \n",
       "heuristics_direct_S_lf_regex_stdtype_basicplus          1210     1123   \n",
       "\n",
       "                                                    Incorrect  Emp. Acc.  \n",
       "dictionary_fuzzy_S_lf_dict_s_type_negs                     74   0.143813  \n",
       "dictionary_fuzzy_S_lf_dict_s_comp_type_negs               292   0.137958  \n",
       "dictionary_direct_S_lf_dict_s_type_negs                    74   0.143813  \n",
       "dictionary_direct_S_lf_dict_s_comp_type_negs              292   0.137958  \n",
       "UMLS_fuzzy_S_lf_PCDS                                       15   0.000000  \n",
       "...                                                       ...        ...  \n",
       "heuristics_direct_S_lf_regex_blinding_negs               2635   0.005389  \n",
       "heuristics_direct_S_lf_regex_stdtype_basicplus_...       2661   0.003277  \n",
       "heuristics_direct_S_lf_lf_lf_s_heurpattern_labe...       2543   0.529336  \n",
       "heuristics_direct_S_lf_regex_blinding                      22   0.972705  \n",
       "heuristics_direct_S_lf_regex_stdtype_basicplus             87   0.928099  \n",
       "\n",
       "[268 rows x 9 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Changed the label mapper for the validation candidates\n",
    "\n",
    "getSummary_and_write(df=val_dict_lfs, df_col_head=val_lf_names, true_labels = np.array(val_y_mapped), file_path= f'/mnt/nas2/results/Results/systematicReview/distant_pico/EBM_PICO_GT/lf_{picos.lower()}_summary_tuipio3_val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9037143b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e855ab57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd91bd4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
